{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urdu_text</th>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ğŸ¤£ğŸ˜‚ğŸ˜‚ ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†Û...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ù†ÛÛŒÚº Ù¾Ø§Ø¦ÛŒÙ† ğŸ˜</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>`` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           urdu_text  is_sarcastic  \\\n",
       "0  ğŸ¤£ğŸ˜‚ğŸ˜‚ ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†Û...           1.0   \n",
       "1  Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ...           1.0   \n",
       "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...           0.0   \n",
       "3                                       Ù†ÛÛŒÚº Ù¾Ø§Ø¦ÛŒÙ† ğŸ˜           0.0   \n",
       "4   `` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ...           1.0   \n",
       "\n",
       "   Unnamed: 2  Unnamed: 3  Unnamed: 4  Unnamed: 5 Unnamed: 6  Unnamed: 7  \n",
       "0         NaN         NaN         NaN         NaN        NaN         NaN  \n",
       "1         NaN         NaN         NaN         NaN        NaN         NaN  \n",
       "2         NaN         NaN         NaN         NaN        NaN         NaN  \n",
       "3         NaN         NaN         NaN         NaN        NaN         NaN  \n",
       "4         NaN         NaN         NaN         NaN        NaN         NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'urdu_sarcastic_dataset.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ø¢Ø¦ÛŒ', 'Ø¢Ø¦Û’', 'Ø¢Ø¬', 'Ø¢Ø®Ø±', 'Ø¢Ø®Ø±Ú©Ø¨Ø±', 'Ø¢Ø¯Ù‡ÛŒ', 'Ø¢Ù‹Ø¨', 'Ø¢Ù¹Ú¾', 'Ø¢ÛŒØ¨', 'Ø§Ø©']\n"
     ]
    }
   ],
   "source": [
    "# Load the stopwords from the text file\n",
    "with open('stopwords-ur.txt', 'r', encoding='utf-8') as file:\n",
    "    urdu_stopwords = file.read().splitlines()\n",
    "\n",
    "# Check the first few stopwords\n",
    "print(urdu_stopwords[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urdu_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ğŸ¤£ğŸ˜‚ğŸ˜‚ ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†Û...</td>\n",
       "      <td>ğŸ¤£ğŸ˜‚ğŸ˜‚ Ù„ÛŒÙ†Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ú©ÙˆØ¬ÛŒ Ù†ÛÛŒÚº Ú†Ø§ÛÛŒÛ’ ğŸ˜ğŸ˜ğŸ˜ğŸ¤£</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ...</td>\n",
       "      <td>Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...</td>\n",
       "      <td>Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ø§Ù¾ÙˆØ²ÛŒØ´...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ù†ÛÛŒÚº Ù¾Ø§Ø¦ÛŒÙ† ğŸ˜</td>\n",
       "      <td>Ù†ÛÛŒÚº Ù¾Ø§Ø¦ÛŒÙ† ğŸ˜</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>`` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ...</td>\n",
       "      <td>`` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø§ÛŒØ³ '' Ø­Ø§Ù…Ø¯ Ù…ÛŒØ±ğŸ˜</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           urdu_text  \\\n",
       "0  ğŸ¤£ğŸ˜‚ğŸ˜‚ ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†Û...   \n",
       "1  Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ...   \n",
       "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...   \n",
       "3                                       Ù†ÛÛŒÚº Ù¾Ø§Ø¦ÛŒÙ† ğŸ˜   \n",
       "4   `` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0      ğŸ¤£ğŸ˜‚ğŸ˜‚ Ù„ÛŒÙ†Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ú©ÙˆØ¬ÛŒ Ù†ÛÛŒÚº Ú†Ø§ÛÛŒÛ’ ğŸ˜ğŸ˜ğŸ˜ğŸ¤£  \n",
       "1  Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ...  \n",
       "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ø§Ù¾ÙˆØ²ÛŒØ´...  \n",
       "3                                       Ù†ÛÛŒÚº Ù¾Ø§Ø¦ÛŒÙ† ğŸ˜  \n",
       "4    `` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø§ÛŒØ³ '' Ø­Ø§Ù…Ø¯ Ù…ÛŒØ±ğŸ˜  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to remove stopwords, handling non-string values\n",
    "def remove_stopwords(text):\n",
    "    if isinstance(text, str):  # Check if the text is a string\n",
    "        words = text.split()  # Split text into words\n",
    "        cleaned_text = ' '.join([word for word in words if word not in urdu_stopwords])\n",
    "        return cleaned_text\n",
    "    else:\n",
    "        return text  # Return the text unchanged if it's not a string\n",
    "\n",
    "# Apply the stopword removal to the 'urdu_text' column\n",
    "df['cleaned_text'] = df['urdu_text'].apply(remove_stopwords)\n",
    "\n",
    "# Display the cleaned text\n",
    "df[['urdu_text', 'cleaned_text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Punctuation, Emojis, and Hashtags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urdu_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ğŸ¤£ğŸ˜‚ğŸ˜‚ ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†Û...</td>\n",
       "      <td>ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†ÛÛŒÚº Ú†...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ...</td>\n",
       "      <td>Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...</td>\n",
       "      <td>Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ù†ÛÛŒÚº Ù¾Ø§Ø¦ÛŒÙ† ğŸ˜</td>\n",
       "      <td>Ù†ÛÛŒÚº Ù¾Ø§Ø¦ÛŒÙ†</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>`` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ...</td>\n",
       "      <td>Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ ØªÚ¾Û’...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           urdu_text  \\\n",
       "0  ğŸ¤£ğŸ˜‚ğŸ˜‚ ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†Û...   \n",
       "1  Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ...   \n",
       "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...   \n",
       "3                                       Ù†ÛÛŒÚº Ù¾Ø§Ø¦ÛŒÙ† ğŸ˜   \n",
       "4   `` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†ÛÛŒÚº Ú†...  \n",
       "1  Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ...  \n",
       "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...  \n",
       "3                                         Ù†ÛÛŒÚº Ù¾Ø§Ø¦ÛŒÙ†  \n",
       "4  Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ ØªÚ¾Û’...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "# Function to clean the text\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # 1. Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        \n",
    "        # 2. Remove Hashtags\n",
    "        text = re.sub(r'#\\w+', '', text)\n",
    "        \n",
    "        # 3. Remove Punctuation\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "        # 4. Remove all emojis\n",
    "        text = re.sub(r'[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FAFF]', '', text)\n",
    "\n",
    "        return text.strip()\n",
    "    else:\n",
    "        return text  # Return unchanged if not a string\n",
    "\n",
    "# Apply the clean_text function to the 'urdu_text' column\n",
    "df['cleaned_text'] = df['urdu_text'].apply(clean_text)\n",
    "\n",
    "# Display the cleaned text\n",
    "df[['urdu_text', 'cleaned_text']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Short Conversations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urdu_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ğŸ¤£ğŸ˜‚ğŸ˜‚ ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†Û...</td>\n",
       "      <td>ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†ÛÛŒÚº Ú†...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ...</td>\n",
       "      <td>Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...</td>\n",
       "      <td>Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ù†ÛÛŒÚº Ù¾Ø§Ø¦ÛŒÙ† ğŸ˜</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>`` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ...</td>\n",
       "      <td>Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ ØªÚ¾Û’...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           urdu_text  \\\n",
       "0  ğŸ¤£ğŸ˜‚ğŸ˜‚ ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†Û...   \n",
       "1  Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ...   \n",
       "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...   \n",
       "3                                       Ù†ÛÛŒÚº Ù¾Ø§Ø¦ÛŒÙ† ğŸ˜   \n",
       "4   `` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†ÛÛŒÚº Ú†...  \n",
       "1  Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ...  \n",
       "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...  \n",
       "3                                                     \n",
       "4  Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ ØªÚ¾Û’...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to filter short conversations\n",
    "def filter_short_conversations(text):\n",
    "    if isinstance(text, str):\n",
    "        # Count the number of words\n",
    "        word_count = len(text.split())\n",
    "        # Return the text if it has 3 or more words, otherwise return an empty string\n",
    "        return text if word_count >= 3 else ''\n",
    "    return text  # Return unchanged if not a string\n",
    "\n",
    "# Apply the filter_short_conversations function to the 'cleaned_text' column\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(filter_short_conversations)\n",
    "\n",
    "# Save the cleaned text to a separate CSV file\n",
    "df[['cleaned_text']].to_csv('cleaned_urdu_text.csv', index=False, encoding='utf-8')\n",
    "# Display the results\n",
    "df[['urdu_text', 'cleaned_text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Some additional preprocessing techniques (normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-29 22:12:52.449409: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-29 22:12:52.453762: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-29 22:12:52.547268: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-29 22:12:52.548838: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-29 22:12:54.570750: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/waghib/Desktop/NLP/Automatic-Sentiment-Analysis-Tool-for-Urdu-Text/nlp_assignment/lib/python3.8/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†ÛÛŒÚº Ú†...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ ØªÚ¾Û’...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        cleaned_text\n",
       "0  ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†ÛÛŒÚº Ú†...\n",
       "1  Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ...\n",
       "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...\n",
       "3                                                   \n",
       "4  Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ ØªÚ¾Û’..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Given text str, replace one or more spacings with a single space, \n",
    "# and one or more linebreaks with a single newline. \n",
    "\n",
    "from urduhack.preprocessing import normalize_whitespace\n",
    "\n",
    "# Apply normalization to the 'cleaned_text' column\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(lambda x: normalize_whitespace(x) if pd.notnull(x) else x)\n",
    "\n",
    "\n",
    "# Save the updated data to a new CSV file\n",
    "df[['cleaned_text']].to_csv('normalized_dataset.csv', index=False, encoding='utf-8')\n",
    "\n",
    "# Display the normalized text\n",
    "df[['cleaned_text']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†ÛÛŒÚº Ú†...\n",
      "1    Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ...\n",
      "2    Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...\n",
      "3                                                     \n",
      "4    Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ ØªÚ¾Û’...\n",
      "Name: cleaned_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Removing some of the additional urdu punctuations from the text\n",
    "\n",
    "from urduhack.preprocessing import remove_punctuation\n",
    "\n",
    "# Ensure all entries in 'cleaned_text' are strings. Replace non-string entries with an empty string.\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(lambda x: str(x) if isinstance(x, str) else '')\n",
    "\n",
    "# Apply remove_punctuation function\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(remove_punctuation)\n",
    "\n",
    "\n",
    "# Save the updated data to a new CSV file\n",
    "df[['cleaned_text']].to_csv('normalized_dataset.csv', index=False, encoding='utf-8')\n",
    "\n",
    "# Display the first 20 rows of 'cleaned_text'\n",
    "print(df['cleaned_text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†ÛÛŒÚº Ú†...\n",
      "1    Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ...\n",
      "2    Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...\n",
      "3                                                     \n",
      "4    Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ ØªÚ¾Û’...\n",
      "Name: cleaned_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Remove accents from any accented unicode characters in text str, either by transforming them \n",
    "# into ascii equivalents or removing them entirely.\n",
    "\n",
    "from urduhack.preprocessing import remove_accents\n",
    "\n",
    "# Ensure all entries in 'cleaned_text' are strings. Replace non-string entries with an empty string.\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(lambda x: str(x) if isinstance(x, str) else '')\n",
    "\n",
    "# Apply remove_punctuation function\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(remove_accents)\n",
    "\n",
    "\n",
    "# Save the updated data to a new CSV file\n",
    "df[['cleaned_text']].to_csv('normalized_dataset.csv', index=False, encoding='utf-8')\n",
    "\n",
    "# Display the first 20 rows of 'cleaned_text'\n",
    "print(df['cleaned_text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/waghib/Desktop/NLP/Automatic-Sentiment-Analysis-Tool-for-Urdu-Text/nlp_assignment/lib/python3.8/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†ÛÛŒÚº Ú†...\n",
      "1    Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ...\n",
      "2    Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...\n",
      "3                                                     \n",
      "4    Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ ØªÚ¾Û’...\n",
      "Name: cleaned_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# This function replaces English digits with Urdu digits.\n",
    "from LughaatNLP import LughaatNLP\n",
    "\n",
    "# Initialize the LughaatNLP object\n",
    "urdu_text_processing = LughaatNLP()\n",
    "\n",
    "# Apply the replace_digits function to each row in the 'cleaned_text' column\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(lambda x: urdu_text_processing.replace_digits(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Save the updated data to a new CSV file\n",
    "df[['cleaned_text']].to_csv('normalized_dataset.csv', index=False, encoding='utf-8')\n",
    "\n",
    "# Display the result 'cleaned_text'\n",
    "print(df['cleaned_text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†ÛÛŒÚº Ú†...\n",
      "1    Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ...\n",
      "2    Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø§Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...\n",
      "3                                                     \n",
      "4    Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø§Ø¦ÛŒ Ø§ÛŒØ³ Ø§Ø¦ÛŒ ØªÚ¾Û’...\n",
      "Name: cleaned_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# This function removes all non-Urdu characters, numbers, and special characters, just leaving only pure \n",
    "# Urdu text even not special character used in urdu.\n",
    "\n",
    "# Apply the just_urdu function to each row in the 'cleaned_text' column\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(lambda x: urdu_text_processing.just_urdu(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Save the updated data to a new CSV file\n",
    "df[['cleaned_text']].to_csv('normalized_dataset.csv', index=False, encoding='utf-8')\n",
    "\n",
    "# Display the result 'cleaned_text'\n",
    "print(df['cleaned_text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # spell checker\n",
    "\n",
    "# # Initialize the LughaatNLP object\n",
    "# spell_checker = LughaatNLP()\n",
    "\n",
    "# # Apply the spell_check function to each row in the 'cleaned_text' column\n",
    "# df['cleaned_text'] = df['cleaned_text'].apply(lambda x: spell_checker.corrected_sentence_spelling(x, 60) if isinstance(x, str) else x)\n",
    "\n",
    "# # Save the updated data to a new CSV file\n",
    "# df[['cleaned_text']].to_csv('normalized_dataset.csv', index=False, encoding='utf-8')\n",
    "\n",
    "# # Display the result 'cleaned_text'\n",
    "# print(df['cleaned_text'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Stemming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    ÛÙˆ Ù„ÛŒÙ†Û Ø¯Û Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ Ú©ÙˆØ¬ÛŒ Ù†ÛØ§ Ú†Ø§ÛÛŒÛ\n",
      "1    Ú†Ù„ Ù…ÛÙ…Ø§Ù†Ø§ Ù…Ø§ Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†Ø§ Ø¯Ø³Ø¯ÛŒ Ø§Úº Ù…Ø§\n",
      "2    Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø§Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...\n",
      "3                                                     \n",
      "4    Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û Ø¨Ú¾ÛŒØ³ Ù…Ø§ ÚˆÛŒ Ø¬ÛŒ Ø§Ø¦ÛŒ Ø§ÛŒØ³ Ø§Ø¦ÛŒ ØªÚ¾Û ...\n",
      "Name: stemmed_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Import LughaatNLP for stemming and lemmatization\n",
    "\n",
    "# Function to apply stemming only\n",
    "def apply_stemming(text):\n",
    "    if isinstance(text, str):\n",
    "        # Apply stemming\n",
    "        stemmed_text = urdu_text_processing.urdu_stemmer(text)\n",
    "        return stemmed_text\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "# Apply stemming to the 'cleaned_text' column\n",
    "df['stemmed_text'] = df['cleaned_text'].apply(apply_stemming)\n",
    "\n",
    "# Save the stemmed output to a new CSV file\n",
    "df[['stemmed_text']].to_csv('stemmed_dataset.csv', index=False, encoding='utf-8')\n",
    "\n",
    "# Display the first 20 rows of 'stemmed_text'\n",
    "print(df['stemmed_text'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Lemmatization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    ÛÙˆÙ†Ø§ Ù„ÛŒÙ†Ø§ Ø¯ÛŒÙ†Ø§ Ù…ÛŒØ±Ø§ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÙˆÙ†Ø§ Ú©ÙˆØ¬ÛŒ ...\n",
      "1    Ú†Ù„Ù†Ø§ Ù…ÛÙ…Ø§Ù† Ù…ÛŒÚº Ú©Ú¾Ø§ Ø³Ø±Ø§ Ú©Ø±Ù†Ø§ Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ...\n",
      "2    Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø§Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ù†Ø§ Ø¬Ø§Ù†Ø§ Ø§Ù¾...\n",
      "3                                                     \n",
      "4    Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Ù… Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒÙ†Ø§ Ø§Ø¦ÛŒ Ø§ÛŒØ³ Ø§Ø¦ÛŒ Øª...\n",
      "Name: lemmatized_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Function to apply lemmatization only\n",
    "def apply_lemmatization(text):\n",
    "    if isinstance(text, str):\n",
    "        # Apply lemmatization\n",
    "        lemmatized_text = urdu_text_processing.lemmatize_sentence(text)\n",
    "        return lemmatized_text\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "# Apply lemmatization to the 'cleaned_text' column\n",
    "df['lemmatized_text'] = df['cleaned_text'].apply(apply_lemmatization)\n",
    "\n",
    "# Save the lemmatized output to a new CSV file\n",
    "df[['lemmatized_text']].to_csv('lemmatized_dataset.csv', index=False, encoding='utf-8')\n",
    "\n",
    "# Display the first 20 rows of 'lemmatized_text'\n",
    "print(df['lemmatized_text'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [ÛÙˆÙ†Ø§, Ù„ÛŒÙ†Ø§, Ø¯ÛŒÙ†Ø§, Ù…ÛŒØ±Ø§, Ø´Ø§Ø¯ÛŒ, ÙØ³Ø§Ø¯Ù†, Ù¹Ú¾ÛŒÚ©, ÛÙˆ...\n",
      "1    [Ú†Ù„Ù†Ø§, Ù…ÛÙ…Ø§Ù†, Ù…ÛŒÚº, Ú©Ú¾Ø§, Ø³Ø±Ø§, Ú©Ø±Ù†Ø§, Ú†Ú‘ÛŒÙ„, Ú†Ø§Ú†ÛŒ,...\n",
      "2    [Ú©Ø§Ù…Ø±Ø§Ù†, Ø®Ø§Ù†, Ø§Ù¾Ú©ÛŒ, Ø¯Ù†, Ø¨Ú¾Ø±ÛŒÛ, Ø²Ù…Û, Ø¯Ø§Ø±ÛŒ, Ù„Ú¯Ù†Ø§...\n",
      "3                                                   []\n",
      "4    [Ù…Ø±Ø§Ø¯, Ø¹Ù„ÛŒ, Ø´Ø§Û, Ú©Ù…, Ø¨Ú¾ÛŒØ³, Ù…ÛŒÚº, ÚˆÛŒ, Ø¬ÛŒÙ†Ø§, Ø§Ø¦ÛŒ,...\n",
      "Name: tokenized_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# This function tokenizes the Urdu text into individual tokens\n",
    "\n",
    "# Apply the urdu_tokenize function to each row in the 'lemmatized_text' column\n",
    "df['tokenized_text'] = df['lemmatized_text'].apply(\n",
    "    lambda x: urdu_text_processing.urdu_tokenize(x) if isinstance(x, str) else x\n",
    ")\n",
    "\n",
    "# Save the tokenized data to a new CSV file\n",
    "df[['tokenized_text']].to_csv('tokenized_dataset.csv', index=False, encoding='utf-8')\n",
    "\n",
    "# Display the first 5 rows of the tokenized text\n",
    "print(df['tokenized_text'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tf-IDF (Term Frequency-Inverse Document Frequency):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words with highest TF-IDF scores:\n",
      "ÛÙˆÙ†Ø§    1379.796968\n",
      "Ù…ÛŒÚº     1146.799896\n",
      "Ú©Ù…       962.396668\n",
      "Ú©Ø±Ù†Ø§     619.291894\n",
      "Ú©Ùˆ       526.431342\n",
      "Ø³Û’       516.883089\n",
      "Ù†ÛÛŒÚº     498.592380\n",
      "Ú©Ø§       477.211109\n",
      "Ø¨Ú¾ÛŒ      446.767171\n",
      "Ø§ÙˆØ±      441.691920\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Join the tokenized text into a single string for each document\n",
    "df['joined_text'] = df['tokenized_text'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the joined text to compute TF-IDF scores\n",
    "tfidf_matrix = vectorizer.fit_transform(df['joined_text'])\n",
    "\n",
    "# Get the words corresponding to the features\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "# Sum the TF-IDF scores for each word across all documents\n",
    "tfidf_sum = tfidf_df.sum().sort_values(ascending=False)\n",
    "\n",
    "# Get the top 10 words with the highest TF-IDF scores\n",
    "top_tfidf_words = tfidf_sum.head(10)\n",
    "\n",
    "# Display the top 10 words and their TF-IDF scores\n",
    "print(\"Top 10 words with highest TF-IDF scores:\")\n",
    "print(top_tfidf_words)\n",
    "\n",
    "# Save the top TF-IDF words to a CSV file\n",
    "top_tfidf_words.to_csv('top_tfidf_words.csv', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Word2Vec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 words most similar to 'Ø§Ú†Ú¾Ø§': [('Ù…Ø´Ú©Ù„', 0.9648818969726562), ('Ø§ÙØ³ÙˆØ³', 0.9622622132301331), ('Ø§Ú†Ú¾ÛŒ', 0.960913896560669), ('ÛÙ…ÛŒÚº', 0.959526002407074), ('Ù…Ø¬Ú¾Û’', 0.9595121145248413)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load the fixed tokenized dataset\n",
    "tokenized_sentences = df['tokenized_text'].tolist()\n",
    "\n",
    "# Train a Word2Vec model\n",
    "# Parameters: \n",
    "#   size=100: Vector size (embedding dimension)\n",
    "#   window=5: Context window size\n",
    "#   min_count=1: Ignores words with frequency lower than this\n",
    "word2vec_model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Save the trained Word2Vec model for future use\n",
    "word2vec_model.save(\"urdu_word2vec_fixed.model\")\n",
    "\n",
    "# finding similar words for \"Ø§Ú†Ú¾Ø§\"\n",
    "try:\n",
    "    similar_words = word2vec_model.wv.most_similar(\"Ø§Ú†Ú¾Ø§\", topn=5)\n",
    "    print(\"Top 5 words most similar to 'Ø§Ú†Ú¾Ø§':\", similar_words)\n",
    "\n",
    "    # Save the similar words to a CSV file\n",
    "    similar_words_df = pd.DataFrame(similar_words, columns=['Word', 'Similarity'])\n",
    "    similar_words_df.to_csv('similar_words_acha.csv', index=False, encoding='utf-8')\n",
    "\n",
    "except KeyError:\n",
    "    print(\"Word 'Ø§Ú†Ú¾Ø§' not found in vocabulary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4: N-grams Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Unigram, Bigram, and Trigram Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Bigrams:\n",
      "('ÛÙˆÙ†Ø§', 'Ù…ÛŒÚº'): 1863\n",
      "('Ø±ÛÙ†Ø§', 'ÛÙˆÙ†Ø§'): 1607\n",
      "('ÛÙˆÙ†Ø§', 'ÛÙˆÙ†Ø§'): 1015\n",
      "('ÛÙˆÙ†Ø§', 'Ú©ÛÙ†Ø§'): 992\n",
      "('ÛÙˆÙ†Ø§', 'Øª'): 807\n",
      "('ÛÙˆÙ†Ø§', 'Ø§ÙˆØ±'): 775\n",
      "('Ú©Ø±Ù†Ø§', 'ÛÙˆÙ†Ø§'): 745\n",
      "('Ù…ÛŒÚº', 'Ù†Û’'): 693\n",
      "('Ù†ÛÛŒÚº', 'ÛÙˆÙ†Ø§'): 618\n",
      "('Ø¬Ø§', 'ÛÙˆÙ†Ø§'): 602\n",
      "\n",
      "Top 10 Trigrams:\n",
      "('Ú©Ø±Ù†Ø§', 'Ø±ÛÙ†Ø§', 'ÛÙˆÙ†Ø§'): 337\n",
      "('ÛÙˆÙ†Ø§', 'Ú©ÛÙ†Ø§', 'Ù…ÛŒÚº'): 190\n",
      "('Ø±ÛÙ†Ø§', 'ÛÙˆÙ†Ø§', 'Ù…ÛŒÚº'): 163\n",
      "('ÛÙˆÙ†Ø§', 'Ø¬Ø§Ù†Ø§', 'ÛÙˆÙ†Ø§'): 141\n",
      "('ÛÙˆÙ†Ø§', 'Øª', 'Ù…ÛŒÚº'): 126\n",
      "('ÛÙˆÙ†Ø§', 'Ù…ÛŒÚº', 'Ù†Û’'): 124\n",
      "('Ù†ÙˆØ§Ø²', 'Ø´Ø±ÛŒÙ', 'Ú©Ù…'): 122\n",
      "('Ø§Ø¦ÛŒ', 'Ø¬ÛŒÙ†Ø§', 'Ø³Ù†Ø¯Ú¾'): 118\n",
      "('Ù¾ÛŒÙ†Ø§', 'Ù¹ÛŒ', 'Ø§Ø¦ÛŒ'): 115\n",
      "('ÛÙˆÙ†Ø§', 'Ø¬Ø§', 'ÛÙˆÙ†Ø§'): 112\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Load the tokenized dataset\n",
    "df_tokenized = pd.read_csv('tokenized_dataset.csv')\n",
    "\n",
    "# Extract the tokenized text column\n",
    "tokenized_sentences = df_tokenized['tokenized_text'].apply(eval).tolist()  # Converting string representation of list to list\n",
    "\n",
    "# Flatten the list of tokenized sentences for unigram analysis\n",
    "all_tokens = [token for sentence in tokenized_sentences for token in sentence]\n",
    "\n",
    "# Generate unigrams, bigrams, and trigrams\n",
    "unigrams = all_tokens\n",
    "bigrams = list(ngrams(all_tokens, 2))\n",
    "trigrams = list(ngrams(all_tokens, 3))\n",
    "\n",
    "# Calculate the frequency distribution of bigrams and trigrams\n",
    "bigram_freq = FreqDist(bigrams)\n",
    "trigram_freq = FreqDist(trigrams)\n",
    "\n",
    "# Get the top 10 most common bigrams and trigrams\n",
    "top_10_bigrams = bigram_freq.most_common(10)\n",
    "top_10_trigrams = trigram_freq.most_common(10)\n",
    "\n",
    "# Display the top 10 bigrams and trigrams\n",
    "print(\"Top 10 Bigrams:\")\n",
    "for bigram, freq in top_10_bigrams:\n",
    "    print(f\"{bigram}: {freq}\")\n",
    "\n",
    "print(\"\\nTop 10 Trigrams:\")\n",
    "for trigram, freq in top_10_trigrams:\n",
    "    print(f\"{trigram}: {freq}\")\n",
    "\n",
    "# Save the bigrams and trigrams to CSV files\n",
    "bigrams_df = pd.DataFrame(top_10_bigrams, columns=['Bigram', 'Frequency'])\n",
    "bigrams_df.to_csv('top_10_bigrams.csv', index=False, encoding='utf-8')\n",
    "\n",
    "trigrams_df = pd.DataFrame(top_10_trigrams, columns=['Trigram', 'Frequency'])\n",
    "trigrams_df.to_csv('top_10_trigrams.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 5: Sentiment Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Model Building:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [-0.17241704, 0.3244016, 0.20045978, 0.4228485...\n",
      "1    [-0.22701998, 0.3297415, -0.0068548904, 0.1310...\n",
      "2    [-0.15025176, 0.2084173, -0.031012284, 0.14914...\n",
      "3    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "4    [-0.41087872, 0.62309057, 0.036108177, -0.1755...\n",
      "Name: sentence_vector, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load the Word2Vec model\n",
    "word2vec_model = Word2Vec.load(\"urdu_word2vec_fixed.model\")\n",
    "\n",
    "def sentence_to_vector(tokens):\n",
    "    # Use tokens directly (assumed to be a list)\n",
    "    vectors = []\n",
    "    \n",
    "    for word in tokens:\n",
    "        if word in word2vec_model.wv:\n",
    "            vectors.append(word2vec_model.wv[word])\n",
    "    \n",
    "    if vectors:  # Return the average vector if there are valid vectors\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(word2vec_model.vector_size)  # Return a zero vector if no valid words\n",
    "\n",
    "# Create a new column for sentence vectors directly from the tokenized_text\n",
    "df['sentence_vector'] = df['tokenized_text'].apply(sentence_to_vector)\n",
    "\n",
    "# Display the first few sentence vectors\n",
    "print(df['sentence_vector'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found NaN values in the is_sarcastic column. Dropping those rows.\n",
      "Accuracy: 0.7080729817545613\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.72      0.72      0.72      2073\n",
      "         1.0       0.70      0.70      0.70      1928\n",
      "\n",
      "    accuracy                           0.71      4001\n",
      "   macro avg       0.71      0.71      0.71      4001\n",
      "weighted avg       0.71      0.71      0.71      4001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# APplying logistic regression\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the Word2Vec model\n",
    "word2vec_model = Word2Vec.load(\"urdu_word2vec_fixed.model\")\n",
    "\n",
    "def sentence_to_vector(tokens):\n",
    "    vectors = []\n",
    "    \n",
    "    for word in tokens:\n",
    "        if word in word2vec_model.wv:\n",
    "            vectors.append(word2vec_model.wv[word])\n",
    "    \n",
    "    if vectors:  # Return the average vector if there are valid vectors\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(word2vec_model.vector_size)  # Return a zero vector if no valid words\n",
    "\n",
    "# Create a new column for sentence vectors directly from the tokenized_text\n",
    "df['sentence_vector'] = df['tokenized_text'].apply(sentence_to_vector)\n",
    "\n",
    "# Check for NaN values in the target column\n",
    "if df['is_sarcastic'].isnull().any():\n",
    "    print(\"Found NaN values in the is_sarcastic column. Dropping those rows.\")\n",
    "    df = df.dropna(subset=['is_sarcastic'])\n",
    "\n",
    "# Prepare the features and labels\n",
    "X = np.array(df['sentence_vector'].tolist())\n",
    "y = df['is_sarcastic'].values  # Use the correct column for labels\n",
    "\n",
    "# Check for NaN values in the features\n",
    "if np.isnan(X).any() or np.isnan(y).any():\n",
    "    print(\"Found NaN values in the features or target. Removing rows with NaN values.\")\n",
    "    valid_indices = ~np.isnan(X).any(axis=1) & ~np.isnan(y)\n",
    "    X = X[valid_indices]\n",
    "    y = y[valid_indices]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the logistic regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 6: Evaluation & Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Evaluation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Performance Metrics\n",
    "After running your logistic regression model on the Urdu posts, you achieved the following evaluation metrics:\n",
    "\n",
    "**Accuracy:** 70.81%\n",
    "\n",
    "**Precision:**\n",
    "- Class 0 (non-sarcastic): 72%\n",
    "- Class 1 (sarcastic): 70%\n",
    "\n",
    "**Recall:**\n",
    "- Class 0 (non-sarcastic): 72%\n",
    "- Class 1 (sarcastic): 70%\n",
    "\n",
    "**F1-Score:**\n",
    "- Class 0 (non-sarcastic): 72%\n",
    "- Class 1 (sarcastic): 70%\n",
    "\n",
    "### Analysis of Performance\n",
    "\n",
    "**Strong Performance Areas:**\n",
    "- **Basic Sentences:** The model performs relatively well on straightforward, structured sentences that do not contain idiomatic expressions or complex syntax.\n",
    "- **Common Sarcasm Indicators:** The model is likely able to recognize certain phrases or words that are commonly associated with sarcasm, leading to better precision and recall in those instances.\n",
    "\n",
    "**Struggling Areas:**\n",
    "- **Complex Sentences:** The model may struggle with more complex sentence structures or those containing multiple clauses, leading to a decrease in recall for sarcastic posts.\n",
    "- **Colloquial Language and Slang:** Informal language, slang, or regional dialects might not be well-represented in the training data, leading to misclassifications.\n",
    "- **Contextual Understanding:** Sarcasm often relies heavily on context, tone, and cultural nuances, which may not be fully captured by the model.\n",
    "- **Negations and Inversion:** Sentences where negations are present can create confusion for the model, affecting its ability to classify sentiment accurately.\n",
    "\n",
    "### Areas for Improvement\n",
    "- **Data Augmentation:** Enhance the dataset with more examples of complex sentences and sarcastic expressions.\n",
    "- **Feature Engineering:** Explore features that capture contextual meaning, such as part-of-speech tagging or dependency parsing.\n",
    "- **Advanced Models:** Consider using advanced machine learning models or deep learning approaches like LSTMs or transformers that can handle context better.\n",
    "- **Fine-Tuning:** Experiment with hyperparameters and explore the use of pre-trained language models for Urdu, if available.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Challenges in Urdu Sentiment Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Challenges\n",
    "### Complex Morphology:\n",
    "\n",
    "- **Word Formation:** Urdu has a rich morphological structure with words often formed through the combination of root words, prefixes, and suffixes. This complexity makes tokenization and normalization challenging.\n",
    "- **Inflection:** The inflectional nature of Urdu can lead to multiple forms of the same word, complicating feature extraction.\n",
    "\n",
    "### Colloquial Language:\n",
    "\n",
    "- **Slang and Dialects:** Urdu spoken in different regions may vary significantly, incorporating local slang and idiomatic expressions that are not present in formal written Urdu.\n",
    "- **Code-Switching:** Many Urdu speakers mix English and Urdu in their posts, which can further complicate sentiment analysis if the model is not trained on such data.\n",
    "\n",
    "### Noisy Data from Social Media:\n",
    "\n",
    "- **Informal Text:** Posts on social media often contain abbreviations, typos, and non-standard grammar, which can hinder the model's ability to parse and understand the text accurately.\n",
    "- **Emotionally Charged Language:** Users may express sentiments in varied ways, including sarcasm, irony, or hyperbole, making it challenging for models to accurately classify emotions.\n",
    "\n",
    "### Lack of Resources:\n",
    "\n",
    "- **Limited Datasets:** High-quality labeled datasets for Urdu sentiment analysis are scarce, which can limit the model's training effectiveness.\n",
    "- **Tools and Libraries:** There may be fewer NLP libraries and tools available for Urdu compared to languages like English, limiting preprocessing and analysis capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ‰ The End ğŸ‰<div style=\"text-align: center;\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
